KNN
1.Introduction:
The KNN algorithm is a robust and versatile classifier that is often used as a benchmark for more complex classifiers such as Artificial Neural Networks (ANN) and Support Vector Machines (SVM). Despite its simplicity, KNN can outperform more powerful classifiers and is used in a variety of applications such as economic forecasting, data compression and genetics.
2.KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations and would like to capture the relationship between and . More formally, our goal is to learn a function so that given an unseen observation , can confidently predict the corresponding output .
3.The KNN classifier is also a non parametric and instance-based learning algorithm.
A:KNN is also a non-parametric learning algorithm because it doesn’t assume anything about the underlying data.
B:Instance-based learning means that our algorithm doesn’t explicitly learn a model. Instead, it chooses to memorize the training instances which are subsequently used as “knowledge” for the prediction phase. Concretely, this means that only when a query to our database is made (i.e. when we ask it to predict a label given an input), will the algorithm use the training instances to spit out an answer.
4.More formally, given a positive integer K, an unseen observation and a similarity metric , KNN classifier performs the following two steps:
It runs through the whole dataset computing between and each training observation. We’ll call the K points in the training data that are closest to the set . Note that K is usually odd to prevent tie situations.
It then estimates the conditional probability for each class, that is, the fraction of points in with that given class label. 
Finally, our input gets assigned to the class with the largest probability.
5.When K is small, we are restraining the region of a given prediction and forcing our classifier to be “more blind” to the overall distribution. A small value for K provides the most flexible fit, which will have low bias but high variance. Graphically, our decision boundary will be more jagged. On the other hand, a higher K averages more voters in each prediction and hence is more resilient to outliers. Larger values of K will have smoother decision boundaries which means lower variance but increased bias.
6.
  
7.Pros of KNN
Simple to implement
Flexible to feature/distance choices
Naturally handles multi-class cases
Can do well in practice with enough representative data
8.Cons of KNN
Need to determine the value of parameter K (number of nearest neighbors)
Computation cost is quite high because we need to compute the distance of each query instance to all training samples.
Storage of data
Must know we have a meaningful distance function.
8.Mathematics:
In KNN, there are a few hyper-parameters that we need to tune to get an optimal result. Among the various hyper-parameters that can be tuned to make the KNN algorithm more effective and reliable, the distance metric is one of the important ones through which we calculate the distance between the data points as for some applications certain distance metrics are more effective. There are many kinds of distance functions that can be used in KNN such as Euclidean Distance, Hamming distance, Minkowski distance.

Euclidean distance can be generalised using Minkowski norm also known as the p norm. The formula for Minkowski distance is: 
D(x, y) = p??d|xd – yd|p 

? Here we can see that the formula differs from the formula of Euclidean distance as we can see that instead of squaring the difference, we have raised the difference to the power of p and have also taken the p root of the difference. Now the biggest advantage of using such a distance metric is that we can change the value of p to get different types of distance metrics. 

? p = 2 

· If we take the value of p as 2 then we get the Euclidean distance which has been discussed above. 

? p = 1 

· If we set p to 1 then we get a distance function known as the Manhattan distance. 

Columnar:
A library for creating columnar output strings using data as input.
columnar() Arguments:
data
An iterable of iterables, typically a list of lists of strings where each string will occupy its own cell in the table. However, list elements need not be strings. No matter what is passed, each element in the list is converted to a string using str().
headers=None
A list of strings, one for each column in data, which will be displayed as the table headers. If left as None this will produce a table that does not have a header row.
no_borders=False
Accepts a boolean value that specifies whether or not to display the borders between rows and columns. Passing True will hide all the borders and convert the headers to all caps for a more minimalistic look.

